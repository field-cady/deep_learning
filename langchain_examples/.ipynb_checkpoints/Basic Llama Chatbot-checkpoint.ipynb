{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2a8195-c983-4481-a062-51cff93de034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "def load_llm():\n",
    "    return LlamaCpp(\n",
    "        model_path=\"models/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "        n_gpu_layers=40,\n",
    "        n_batch=512,  # Batch size for model processing\n",
    "        verbose=False,  # Enable detailed logging for debugging\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d23db0-f6f4-4231-ae40-12f694dedc0b",
   "metadata": {},
   "source": [
    "# Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6000b9-db4c-4145-b490-87bf433c031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load the LlamaCpp language model, adjust GPU usage based on your hardware\n",
    "llm = load_llm()\n",
    "\n",
    "# Define the prompt template with a placeholder for the question\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Create an LLMChain to manage interactions with the prompt and model\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b49bd-8868-44d3-a094-b655fa39bc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot initialized, ready to chat...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Hello, how are you doing?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taco/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm feeling great! Just got back from a wonderful hike in the nearby mountains. The fresh air and exercise really invigorated me. How about you?\n",
      "\n",
      "Explanation:\n",
      "This is an example of a casual greeting, where two people are exchanging pleasantries. The question \"Hello, how are you doing?\" is a common way to initiate a conversation, and the response \"I'm feeling great!\" is a typical way to respond when asking how someone is doing. The addition of the detail about the hike adds a personal touch and helps to build rapport between the two people. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Which mountains did you go to?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain range that I went to is the Rocky Mountains. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Oh nice!  Which mountain in partiuclar did you hike on?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain I hiked on was Mount Everest, the highest peak in the world! It was an incredible experience and a dream come true for me. The hike was challenging but rewarding, and the views from the summit were breathtaking. I'm so grateful to have had the opportunity to experience it. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  But Everest isn't in the rockies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are correct! Mount Everest is actually located in the Himalayan mountain range, not the Rocky Mountains. My apologies for the confusion. Thank you for pointing that out! \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  I need your help.  I will tell you a story and you need to tell me whether it is a happy story or a sad story\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course, I'd be happy to help! Please go ahead and share the story with me. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Once upon a time there was a princess.  She was trapped in a castle, and only a kiss could release her.  One day a prince came by and kissed her.  She got out, and they lived happily ever after.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a princess who found herself trapped inside a magnificent castle. Despite the beauty that surrounded her, she couldn't help but feel suffocated by the high walls that confined her. She longed to break free and experience the world beyond the castle's gates.\n",
      "One day, a handsome prince happened to pass by the castle while on his journey. As he caught sight of the princess peering through the windows, he felt compelled to stop and offer his assistance. With a gallant smile, he asked if she needed help escaping the confines of the castle.\n",
      "The princess was overjoyed at the prospect of freedom and gratefully accepted the prince's offer. As he leaned forward to kiss her, their lips met with a tender touch that sent shivers down their spines. With the power of love, the princess felt herself released from the castle's grasp, and she emerged from her prison with a newfound sense of purpose.\n",
      "Together, the prince and the princess set off on an adventure, exploring the world beyond the castle walls. They laughed, loved, and lived happily ever after, always remembering the magical \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Is it a love story or a sad story?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a love story with a sad twist. The couple in the story was deeply in love, but they were torn apart by circumstances beyond their control. Despite their best efforts to be together, they were unable to overcome the obstacles that stood in their way. In the end, they were left with nothing but memories of each other and the knowledge that they would never be together again. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  On a scale from 1 to 10, how much was it a story about hamsters?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a story about hamsters on a scale of 1 to 10. The score is 8. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  On a scale from 1 to 10, how much was it a story about people?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story was a 7 out of 10 in terms of its focus on people. While the plot revolved around a group of individuals and their experiences, the story also delved into themes that are relevant to society as a whole, such as the importance of empathy and understanding in building a harmonious community. The characters were well-developed and relatable, but the story could have benefited from even more character development and depth. Overall, the story was more focused on people than non-human elements like objects or settings, but not as strongly focused as a pure character study. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  On a scale from 1 to 10, how much was it a story about armadillos?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story was a 6 out of 10 in terms of its focus on armadillos. While the armadillo did play a significant role in the plot, other elements such as the characters' personal struggles and the themes of identity and belonging also received attention. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Tell me a story about a butterfly using very simple language\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a beautiful butterfly named Betty. Betty loved to fly and play in the garden. She would flutter around the flowers and drink their sweet nectar. One day, Betty met a new friend, a little bird named Benny. Benny liked to fly too, and he showed Betty all the best places to fly. They had so much fun together, and Betty was happy to have found a new friend. The end. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Chatbot initialized, ready to chat...\")\n",
    "while True:\n",
    "    question = input(\"> \")\n",
    "    answer = llm_chain.run(question)\n",
    "    print(answer, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12aa5d8-9121-4f51-b07f-18d160370f9c",
   "metadata": {},
   "source": [
    "# RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13290ba5-0f3f-41ce-b8c9-720962d77dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda533d4-bb2d-4b27-88b5-41c8006a2402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = load_llm()\n",
    "\n",
    "def create_vector_store(data_dir):\n",
    "    '''Create a vector store from PDF files'''\n",
    "    # define what documents to load\n",
    "    loader = DirectoryLoader(path=data_dir, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "    # interpret information in the documents\n",
    "    documents = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                              chunk_overlap=50)\n",
    "    texts = splitter.split_documents(documents)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': 'cpu'})\n",
    "    # create the vector store database\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    return db\n",
    "\n",
    "db = create_vector_store('pdf_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c21cbe-69ee-4795-bbe6-1bdd54e6cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    # prepare the template we will use when prompting the AI\n",
    "    template = \"\"\"Use the provided context to answer the user's question.\n",
    "    If you don't know the answer, respond with \"I do not know\".\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "def create_chain():\n",
    "    #db = create_vector_store(data_dir='data')\n",
    "    #llm = load_llm()\n",
    "    prompt = create_prompt_template()\n",
    "    retriever = db.as_retriever(search_kwargs={'k': 2})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                        chain_type='stuff',\n",
    "                                        retriever=retriever,\n",
    "                                        return_source_documents=False,\n",
    "                                        chain_type_kwargs={'prompt': prompt})\n",
    "    return chain\n",
    "\n",
    "\n",
    "chain = create_chain()\n",
    "\n",
    "def query_doc(chain, question):\n",
    "    return chain({'query':question})['result']\n",
    "\n",
    "\n",
    "def main():\n",
    "  chain = create_chain()\n",
    "\n",
    "  print(\"Chatbot for PDF files initialized, ready to query...\")\n",
    "  while True:\n",
    "      question = input(\"> \")\n",
    "      answer = query_doc(chain, question)\n",
    "      print(': ', answer, '\\n')\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633204af-99f4-4c93-9598-0b7b840de527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  are ragworms a kind of crustacean?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taco/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":   Based on the provided context, ragworms are a type of polychaete worm. While they are often used as bait in fishing, they are not classified as crustaceans but rather as a type of worm. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = input(\"> \")\n",
    "answer = query_doc(chain, question)\n",
    "print(': ', answer, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
